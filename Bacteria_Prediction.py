# -*- coding: utf-8 -*-
"""project303.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dPFiQFG_wppX-DzrcgKvSCIU1UXqn0BB
"""

import numpy as np
import pandas as pd
# Read CSV File
df=pd.read_csv('/content/drive/MyDrive/dataset/train.csv')

from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.metrics import f1_score
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn import preprocessing
from sklearn import svm
from sklearn.model_selection import learning_curve
from sklearn.model_selection import StratifiedKFold
from scipy import stats
import seaborn as sns
import statsmodels.api as sm
from sklearn.metrics import log_loss
from sklearn.svm import LinearSVC

def reduce_mem_usage(df, verbose=True):
    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']
    start_mem = df.memory_usage().sum() / 1024**2

    for col in df.columns:
        col_type = df[col].dtypes
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
    end_mem = df.memory_usage().sum() / 1024**2

    if verbose:
        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))
 
    return df
print("Train data")
train_data = reduce_mem_usage(df)

df.head()
df.shape

#drop row_id
df.drop("row_id", axis = 1, inplace = True)

#find the unique elements of an array
df["target"].unique()

df['target'].hist(bins=50,figsize=(25,5),alpha=0.6)
plt.show()

df['target'].value_counts()

#df1= df.drop(["target"], axis = 1)
df.head()

for i in range(df.shape[1]):
    n_miss = df.iloc[:, i].isnull().sum()
    perc = n_miss / df.shape[0] * 100
    print('> %s, Missing: %d (%.1f%%)' % (df.columns[i], n_miss, perc))

print(f"Total number of duplicated rows: {df.duplicated().sum()} out of {df.shape[0]} ({df.duplicated().sum()/df.shape[0]*100:.2f}%)")
train_data = df.drop_duplicates()
print(f"Total number of rows after removal: {train_data.shape[0]}")

df.head()

#fill null value with median
df.fillna(df.median())

# Check again for missing values 

print(df.isnull())

# Check the shape of the dataset

print(df.shape)

df.describe()

#describing by target
df.groupby("target").describe()

table = df.groupby("target").sum().values
print(table)

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split

#declaring dependent and independent values
X = df.drop("target", axis = 1)
y = df["target"]
dc=df.copy()

sns.scatterplot(x="A10T0G0C0",
                    y="target", 
                    data=dc)

dc

y

#normalize labels(lebel classes between 0 & n-1)
le = LabelEncoder()
y = le.fit_transform(y)

print(type(y))

#normalize y
normalized_arr = preprocessing.normalize([y])

normalized_arr

print(normalized_arr.shape)

#for printing full numpy array
import sys
np.set_printoptions(threshold=sys.maxsize)

#removing outliers
from sklearn.preprocessing import RobustScaler
from sklearn import preprocessing
rs = preprocessing.RobustScaler()
X1=rs.fit_transform(X)

X1

#scaling the unit variance
ss = StandardScaler()
X = ss.fit_transform(X)

#spliting train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 50)

#logistic regression for multi class
log_reg = LogisticRegression(multi_class = "ovr", max_iter = 1000,solver='liblinear', penalty='l1' , C=1.0)

log_reg.fit(X_train, y_train)

y_pred = log_reg.predict(X_test)

#creating confusion matrix
conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)

#ploting confusion matrix
fig, ax = plt.subplots(figsize=(25, 25))
ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')
 
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

#maping the predicted value of target class
prediction = list(map(round,y_pred))
print(prediction)

print('\033[1;35;1mActual class :\033[0m', list(y_test))
print('\033[1;35;1mPredicted class:\033[0m ', prediction)

cm = confusion_matrix (y_test, prediction)
print(cm)

print("\n\033[1;36;1mAccuracy:\033[0m ", metrics.accuracy_score(y_test, y_pred))
print("\n\033[1;36;1mPrecision:\033[0m ", metrics.precision_score(y_test, y_pred,average='macro'))
print("\n\033[1;36;1mRecall:\033[0m ", metrics.recall_score(y_test, y_pred,average='macro'))
print('\n\033[1;36;1mF1 Score:\033[0m ' ,f1_score(y_test, y_pred,average='macro'))

#AUC score
pred=log_reg.predict_proba(X_test)
auc=metrics.roc_auc_score(y_test,pred,multi_class='ovr')
print('\n\033[1;36;1mArea Under Curve:\033[0m ',auc)

#ROC curve
y_pred_proba = log_reg.predict_proba(X_test)[::,1]
fpr,tpr,thresh=roc_curve(y_test,y_pred_proba,pos_label=1)

plt.figure()
plt.plot(fpr,tpr,linestyle='--',color='blue')
plt.plot([0,1],[0,1],'r--')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()

#checking outliers
df1= df.drop(["target"], axis = 1)
y1=df["target"]
for i in df1:
  print(i)
  upper_quartile = df1[i].mean() + 3*df1[i].std()
  lower_quartile = df1[i].mean() - 3*df1[i].std()
  print("Highest allowed: ", upper_quartile)
  print("Lowest allowed: ", lower_quartile)
  outliers = df1[(df1[i] > upper_quartile) | (df1[i] < lower_quartile)]
  print(outliers.shape[0])

ax = sns.heatmap(df1)

#sns.set(rc={"figure.figsize":(30, 10)})
boxplot = sns.boxplot(data=df1)

col=df.columns
count=1
plt.subplots(figsize=(20, 18))
for i in col:
    plt.subplot(6,1,count)
    sns.scatterplot(df["target"],df[i])
    count+=1
    if count==6:
      break

plt.show()

#droping outliers
for j in df1:
     out = pd.concat([outliers]).drop_duplicates()
     df1[j].drop(out.index, axis=0, inplace=True)

"""**Logistic Regression with dataset after dropping outliers, as before**"""

le = LabelEncoder()
y1 = le.fit_transform(y1)

normalized_arr1 = preprocessing.normalize([y1])

X1_train, X1_test, y1_train, y1_test = train_test_split(X, y, test_size = 0.3, random_state = 50)

log_reg1 = LogisticRegression(multi_class = "ovr", max_iter = 1000,solver='liblinear', penalty='l1' , C=1.0)

log_reg1.fit(X1_train, y1_train)

y1_pred = log_reg1.predict(X1_test)

conf_matrix1 = confusion_matrix(y_true=y1_test, y_pred=y1_pred)

fig, ax = plt.subplots(figsize=(25, 25))
ax.matshow(conf_matrix1, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix1.shape[0]):
    for j in range(conf_matrix1.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix1[i, j], va='center', ha='center', size='xx-large')
 
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

prediction1 = list(map(round,y1_pred))
print(prediction1)

print('\033[1;35;1mActual class :\033[0m', list(y1_test))
print('\033[1;35;1mPredicted class:\033[0m ', prediction1)

print("\n\033[1;36;1mAccuracy:\033[0m ", metrics.accuracy_score(y1_test, y1_pred))
print("\n\033[1;36;1mPrecision:\033[0m ", metrics.precision_score(y1_test, y1_pred,average='macro'))
print("\n\033[1;36;1mRecall:\033[0m ", metrics.recall_score(y1_test, y1_pred,average='macro'))
print('\n\033[1;36;1mF1 Score:\033[0m ' ,f1_score(y1_test, y1_pred,average='macro'))

pred=log_reg1.predict_proba(X1_test)
auc=metrics.roc_auc_score(y1_test,pred,multi_class='ovr')
print('\n\033[1;36;1mArea Under Curve:\033[0m ',auc)

y_pred_proba1 = log_reg1.predict_proba(X1_test)[::,1]
fpr,tpr,thresh=roc_curve(y1_test,y_pred_proba1,pos_label=1)

plt.figure()
plt.plot(fpr,tpr,linestyle='--',color='blue')
plt.plot([0,1],[0,1],'r--')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()

#logloss function
logloss = log_loss(y1_test, log_reg1.predict_proba(X1_test))
logloss

import numpy as np
import pandas as pd
# Read CSV File
test=pd.read_csv('/content/drive/MyDrive/dataset/test.csv')

test.head()

row_id_test=test["row_id"]
test.drop("row_id",axis=1,inplace=True)

for i in range(test.shape[1]):
    n_miss = test.iloc[:, i].isnull().sum()
    perc = n_miss / test.shape[0] * 100
    print('> %s, Missing: %d (%.1f%%)' % (test.columns[i], n_miss, perc))

test=ss.fit_transform(test)

y_pred_testdata=log_reg.predict(test)

y_pred_testdata

prediction2 = le.inverse_transform(y_pred_testdata)

print(y_pred_testdata)

row_id_test

bacteria_pred= pd.DataFrame({"row_id":row_id_test,"target":prediction})

bacteria_pred

X=test
y=y_pred_testdata

X2_train, X2_test, y2_train, y2_test = train_test_split(X, y, test_size = 0.3, random_state = 50)

log_reg_test = LogisticRegression(multi_class = "multinomial", max_iter = 1000,solver='lbfgs', penalty='l2' , C=10.0)

log_reg_test.fit(X2_train, y2_train)

y2_pred = log_reg_test.predict(X2_test)

conf_matrix = confusion_matrix(y_true=y2_test, y_pred=y2_pred)

prediction2 = list(map(round,y2_pred))
print(prediction2)

print('\033[1;35;1mActual class :\033[0m', list(y2_test))
print('\033[1;35;1mPredicted class:\033[0m ', prediction2)

cm = confusion_matrix (y2_test, prediction2)
print(cm)

print("\n\033[1;36;1mAccuracy:\033[0m ", metrics.accuracy_score(y2_test, y2_pred))
print("\n\033[1;36;1mPrecision:\033[0m ", metrics.precision_score(y2_test, y2_pred,average='macro'))
print("\n\033[1;36;1mRecall:\033[0m ", metrics.recall_score(y2_test, y2_pred,average='macro'))
print('\n\033[1;36;1mF1 Score:\033[0m ' ,f1_score(y2_test, y2_pred,average='macro'))

pred=log_reg_test.predict_proba(X2_test)
auc=metrics.roc_auc_score(y2_test,pred,multi_class='ovr')
print('\n\033[1;36;1mArea Under Curve:\033[0m ',auc)

y_pred_proba2 = log_reg_test.predict_proba(X2_test)[::,1]
fpr,tpr,thresh=roc_curve(y2_test,y_pred_proba2,pos_label=1)

plt.figure()
plt.plot(fpr,tpr,linestyle='--',color='blue')
plt.plot([0,1],[0,1],'r--')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()

"""## **For taining data, solver= lbfgs, penalty=l2, c=0.1**"""

#checking with training data c=0.1
log_reg1 = LogisticRegression(multi_class = "multinomial", max_iter = 1000,solver='lbfgs', penalty='l2' , C=0.1)

log_reg1.fit(X1_train, y1_train)

y1_pred = log_reg1.predict(X1_test)

conf_matrix1 = confusion_matrix(y_true=y1_test, y_pred=y1_pred)

prediction1 = list(map(round,y1_pred))
print(prediction1)

print('\033[1;35;1mActual class :\033[0m', list(y1_test))
print('\033[1;35;1mPredicted class:\033[0m ', prediction1)

print("\n\033[1;36;1mAccuracy:\033[0m ", metrics.accuracy_score(y1_test, y1_pred))
print("\n\033[1;36;1mPrecision:\033[0m ", metrics.precision_score(y1_test, y1_pred,average='macro'))
print("\n\033[1;36;1mRecall:\033[0m ", metrics.recall_score(y1_test, y1_pred,average='macro'))
print('\n\033[1;36;1mF1 Score:\033[0m ' ,f1_score(y1_test, y1_pred,average='macro'))

pred=log_reg1.predict_proba(X1_test)
auc=metrics.roc_auc_score(y1_test,pred,multi_class='ovr')
print('\n\033[1;36;1mArea Under Curve:\033[0m ',auc)

y_pred_proba1 = log_reg1.predict_proba(X_test)[::,1]
fpr,tpr,thresh=roc_curve(y_test,y_pred_proba1,pos_label=1)

plt.figure()
plt.plot(fpr,tpr,linestyle='--',color='blue')
plt.plot([0,1],[0,1],'r--')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()

"""## **For taining data, solver= lbfgs, penalty=l2, c=10**"""

#checking with training data c=10
log_reg1 = LogisticRegression(multi_class = "multinomial", max_iter = 1000,solver='lbfgs', penalty='l2' , C=10)

log_reg1.fit(X1_train, y1_train)

y1_pred = log_reg1.predict(X1_test)

conf_matrix1 = confusion_matrix(y_true=y1_test, y_pred=y1_pred)

prediction1 = list(map(round,y1_pred))
print(prediction1)

print('\033[1;35;1mActual class :\033[0m', list(y1_test))
print('\033[1;35;1mPredicted class:\033[0m ', prediction1)

print("\n\033[1;36;1mAccuracy:\033[0m ", metrics.accuracy_score(y1_test, y1_pred))
print("\n\033[1;36;1mPrecision:\033[0m ", metrics.precision_score(y1_test, y1_pred,average='macro'))
print("\n\033[1;36;1mRecall:\033[0m ", metrics.recall_score(y1_test, y1_pred,average='macro'))
print('\n\033[1;36;1mF1 Score:\033[0m ' ,f1_score(y1_test, y1_pred,average='macro'))

pred=log_reg1.predict_proba(X1_test)
auc=metrics.roc_auc_score(y1_test,pred,multi_class='ovr')
print('\n\033[1;36;1mArea Under Curve:\033[0m ',auc)

y_pred_proba1 = log_reg1.predict_proba(X1_test)[::,1]
fpr,tpr,thresh=roc_curve(y1_test,y_pred_proba1,pos_label=1)

plt.figure()
plt.plot(fpr,tpr,linestyle='--',color='blue')
plt.plot([0,1],[0,1],'r--')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()

"""**Testing data is working good in case of logistic regression, with C value 10**

**Accuracy:  0.9739666666666666**

**Precision:  0.9735510519645676**

**Recall:  0.9708475789587856**

**F1 Score:  0.972166088128412**

**Area Under Curve:  0.9992719278271087**

**for c=1.0, testing, logistic regression**

## **SVM with C=1, solver= linerSVC**
"""

lsvc = LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,intercept_scaling=1, loss='squared_hinge', max_iter=1000,multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
          verbose=0)

lsvc.fit(X_train, y_train)

Y_pred = lsvc.predict(X_test)

compare_dataset = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': Y_pred.flatten()})

print(compare_dataset)

print("Accuracy: ", metrics.accuracy_score(y_test, Y_pred))
print("Precision: ", metrics.precision_score(y_test, Y_pred,average='weighted'))
print("Recall: ", metrics.recall_score(y_test, Y_pred,average='weighted'))
print('F1 Score: ' ,f1_score(y_test, Y_pred,average='weighted'))

y_pred_train = lsvc.predict(X_train)

print("Training Accuracy: ", metrics.accuracy_score(y_train, y_pred_train))

svm= svm.SVC(kernel='linear', max_iter=1000, random_state=0, gamma=2, C=1, probability=True)

svm.fit(X_train, y_train)

Y_pred = svm.predict(X_test)

pred2=svm.predict_proba(X_test)

auc2=metrics.roc_auc_score(y_test,pred2,multi_class='ovr')
print('Area Under Curve SVM: ',auc2)

fpr2,tpr2,thresh2=roc_curve(y_test,pred2[:,1],pos_label=1)
plt.figure()
plt.plot(fpr2,tpr2,linestyle='--',color='blue')
plt.plot([0,1],[0,1],'r--')
plt.title('ROC Curve of SVM model')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()

"""## **svm with kernel rbf and C=1, random state=0**"""

svm= svm.SVC(kernel='rbf', max_iter=1000, random_state=0, gamma=2, C=1, probability=True)

svm.fit(X_train, y_train)

Y_pred = svm.predict(X_test)

compare_dataset = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': Y_pred.flatten()})

print(compare_dataset)

print("Accuracy: ", metrics.accuracy_score(y_test, Y_pred))
print("Precision: ", metrics.precision_score(y_test, Y_pred,average='weighted'))
print("Recall: ", metrics.recall_score(y_test, Y_pred,average='weighted'))

y_pred_train = svm.predict(X_train)

print("Training Accuracy: ", metrics.accuracy_score(y_train, y_pred_train))

pred2=svm.predict_proba(X_test)

auc2=metrics.roc_auc_score(y_test,pred2,multi_class='ovr')
print('Area Under Curve SVM: ',auc2)

fpr2,tpr2,thresh2=roc_curve(y_test,pred2[:,1],pos_label=1)
plt.figure()
plt.plot(fpr2,tpr2,linestyle='--',color='blue')
plt.plot([0,1],[0,1],'r--')
plt.title('ROC Curve of SVM model')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()

"""## **SVM with kernel=rbf, C=10, random state=77**"""

svm= svm.SVC(kernel='rbf', max_iter=1000, random_state=77, gamma=2, C=10, probability=True)

svm.fit(X_train, y_train)

Y_pred = svm.predict(X_test)

compare_dataset = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': Y_pred.flatten()})

print(compare_dataset)

print("Accuracy: ", metrics.accuracy_score(y_test, Y_pred))
print("Precision: ", metrics.precision_score(y_test, Y_pred,average='weighted'))
print("Recall: ", metrics.recall_score(y_test, Y_pred,average='weighted'))

y_pred_train = svm.predict(X_train)

print("Training Accuracy: ", metrics.accuracy_score(y_train, y_pred_train))

pred3=svm.predict_proba(X_test)

auc2=metrics.roc_auc_score(y_test,pred3,multi_class='ovr')
print('Area Under Curve SVM: ',auc2)

fpr2,tpr2,thresh2=roc_curve(y_test,pred3[:,1],pos_label=1)
plt.figure()
plt.plot(fpr2,tpr2,linestyle='--',color='blue')
plt.plot([0,1],[0,1],'r--')
plt.title('ROC Curve of SVM model')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()

"""## **OLS model**"""

X = np.asarray(X, dtype = float)
Y = np.asarray(y, dtype = float)

X = sm.add_constant(X)

model = sm.OLS(Y, X) 
results = model.fit()
res=results.summary()
print(res)

"""### **SVM for testing dataset, kernel=rbf,C=1**"""

X1=test
y1=y_pred_testdata

X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.3, random_state = 50)

svm= svm.SVC(kernel='rbf', max_iter=1000, random_state=0, gamma=2, C=1, probability=True)

svm.fit(X1_train, y1_train)

Y_pred1 = svm.predict(X1_test)

compare_dataset = pd.DataFrame({'Actual': y1_test.flatten(), 'Predicted': Y_pred1.flatten()})

print(compare_dataset)

print("Accuracy: ", metrics.accuracy_score(y1_test, Y_pred1))
print("Precision: ", metrics.precision_score(y1_test, Y_pred1,average='weighted'))
print("Recall: ", metrics.recall_score(y1_test, Y_pred1,average='weighted'))

y_pred_test = svm.predict(X1_train)

print("Testing Accuracy: ", metrics.accuracy_score(y1_train, y_pred_test))

pred4=svm.predict_proba(X1_test)

auc_test=metrics.roc_auc_score(y1_test,pred4,multi_class='ovr')
print('Area Under Curve SVM: ',auc_test)

fpr3,tpr3,thresh3=roc_curve(y1_test,pred4[:,1],pos_label=1)
plt.figure()
plt.plot(fpr3,tpr3,linestyle='--',color='blue')
plt.plot([0,1],[0,1],'r--')
plt.title('ROC Curve of SVM model')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()

"""### **SVM for testing dataset, kernel=rbf,C=0.1**"""

svm= svm.SVC(kernel='rbf', max_iter=1000, random_state=0, gamma=2, C=0.1, probability=True)

svm.fit(X1_train, y1_train)

Y_pred1 = svm.predict(X1_test)

print("Accuracy: ", metrics.accuracy_score(y1_test, Y_pred1))
print("Precision: ", metrics.precision_score(y1_test, Y_pred1,average='weighted'))
print("Recall: ", metrics.recall_score(y1_test, Y_pred1,average='weighted'))

y_pred_test = svm.predict(X1_train)

print("Testing Accuracy: ", metrics.accuracy_score(y1_train, y_pred_test))

pred4=svm.predict_proba(X1_test)

auc_test=metrics.roc_auc_score(y1_test,pred4,multi_class='ovr')
print('Area Under Curve SVM: ',auc_test)

fpr3,tpr3,thresh3=roc_curve(y1_test,pred4[:,1],pos_label=1)
plt.figure()
plt.plot(fpr3,tpr3,linestyle='--',color='blue')
plt.plot([0,1],[0,1],'r--')
plt.title('ROC Curve of SVM model')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()



